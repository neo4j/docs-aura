[[aurads-load-csv]]
= Loading CSV files
:description: This page describes how to load CSV files into a Neo4j AuraDS instance.
:star: *
:notebook-name: Loading_CSV_files_(GDS_client).ipynb

include::partial$aurads/colab.adoc[]

A CSV file can be loaded into an AuraDS instance using the link:{neo4j-docs-base-uri}/cypher-manual/current/clauses/load-csv/[`LOAD CSV`^] Cypher clause. For security reasons it is not possible to load local CSV files, which must be instead publicly accessible on HTTP or HTTPS servers such as GitHub, Google Drive, and Dropbox. Another way to make CSV files available is to upload them to a cloud bucket storage (such as Google Cloud Storage or Amazon S3) and configure the bucket as a static website.

In this example we will load three CSV files:

* `movies.csv`: a list of movies with their title, release year and a short description
* `people.csv`: a list of actors with their year of birth
* `actors.csv`: a list of acting roles, where actors are matched with the movies they had a role in

WARNING: The `LOAD CSV` command is built to handle small to medium-sized data sets, such as anything up to 10 million nodes and relationships. You should avoid using this command for any data sets exceeding this limit.

include::partial$aurads/setup.adoc[]

== Create constraints

Adding constraints before loading any data usually improves data loading performance. In fact, besides adding an integrity check, a unique constraint adds an index on a property at the same time, so that `MATCH` and `MERGE` operations during loading are faster.

[WARNING]
====
For best performance when using `MERGE` or `MATCH` with `LOAD CSV`, make sure an index or a unique constraint has been created on the property used for merging.
Read the link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/[Cypher documentation] for more information on constraints.
====

In this example we add uniqueness constraints on both movie titles and actors' names.

[.tabbed-example]
====
[.include-with-GDS-client]
=====
[source, python, subs=attributes+]
----
# Make movie titles unique
gds.run_cypher("""
    CREATE CONSTRAINT FOR (movie:Movie) REQUIRE movie.title IS UNIQUE
""")

# Make person names unique
gds.run_cypher("""
    CREATE CONSTRAINT FOR (person:Person) REQUIRE person.name IS UNIQUE
""")
----
=====

[.include-with-Cypher]
=====
[source, cypher, subs=attributes+]
----
CREATE CONSTRAINT FOR (movie:Movie) REQUIRE movie.title IS UNIQUE;
CREATE CONSTRAINT FOR (person:Person) REQUIRE person.name IS UNIQUE;
----
=====

[.include-with-Python-driver]
=====
[source, python, subs=attributes+]
----
movie_title_constraint = """
    CREATE CONSTRAINT FOR (movie:Movie) REQUIRE movie.title IS UNIQUE
"""

person_name_constraint = """
    CREATE CONSTRAINT FOR (person:Person) REQUIRE person.name IS UNIQUE
"""

# Create the driver session
with driver.session() as session:
    # Make movie titles unique
    session.run(movie_title_constraint).data()
    # Make person names unique
    session.run(person_name_constraint).data()
----
=====
====

== Add nodes from CSV files

We are now ready to load the CSV files from their URIs and create nodes from the data they contain. In the following examples, `LOAD CSV` is used with `WITH HEADERS` to access `row` fields by their corresponding column name. Furthermore:

* `MERGE` is used with the indexed properties to take advantage of the constraints created in the <<_create_constraints>> section.
* `ON CREATE SET` is used to set the value of a node property when a new one is created.
* `RETURN count({star})` is used to show the number of processed rows.

Note that the CSV files in this example are curated, so some assumptions are made for simplicity. In a real-world scenario, for example, a CSV file could contain multiple rows that would attempt to assign different property values to the same node; in this case, an link:{neo4j-docs-base-uri}/cypher-manual/current/clauses/merge/#merge-merge-with-on-match[`ON MATCH SET`^] clause must be added to ensure this case is dealt with appropriately.

[.tabbed-example]
====
[.include-with-GDS-client]
=====
[source, python, subs=attributes+]
----
gds.run_cypher("""
    LOAD CSV
      WITH HEADERS
      FROM 'https://data.neo4j.com/intro/movies/movies.csv' AS row
    MERGE (m:Movie {title: row.title})
      ON CREATE SET m.released = toInteger(row.released), m.tagline = row.tagline
    RETURN count({star})
""")

gds.run_cypher("""
    LOAD CSV
      WITH HEADERS
      FROM 'https://data.neo4j.com/intro/movies/people.csv' AS row
    MERGE (p:Person {name: row.name}) 
      ON CREATE SET p.born = toInteger(row.born)
    RETURN count({star})
""")
----
=====

[.include-with-Cypher]
=====
[source, cypher, subs=attributes+]
----
LOAD CSV
  WITH HEADERS
  FROM 'https://data.neo4j.com/intro/movies/movies.csv' AS row
MERGE (m:Movie {title: row.title})
  ON CREATE SET m.released = toInteger(row.released), m.tagline = row.tagline
RETURN count({star});

LOAD CSV
  WITH HEADERS
  FROM 'https://data.neo4j.com/intro/movies/people.csv' AS row
MERGE (p:Person {name: row.name}) 
  ON CREATE SET p.born = toInteger(row.born)
RETURN count({star});
----
=====

[.include-with-Python-driver]
=====
[source, python, subs=attributes+]
----
load_movies_csv = """
    LOAD CSV
      WITH HEADERS
      FROM 'https://data.neo4j.com/intro/movies/movies.csv' AS row
    MERGE (m:Movie {title: row.title})
      ON CREATE SET m.released = toInteger(row.released), m.tagline = row.tagline
    RETURN count({star})
"""

load_people_csv = """
    LOAD CSV
      WITH HEADERS
      FROM 'https://data.neo4j.com/intro/movies/people.csv' AS row
    MERGE (p:Person {name: row.name}) 
      ON CREATE SET p.born = toInteger(row.born)
    RETURN count({star})
"""

# Create the driver session
with driver.session() as session:
    # Load the CSV files
    session.run(load_movies_csv).data()
    session.run(load_people_csv).data()
----
=====
====

== Add relationships from CSV files

Similarly to what we have done for nodes, we now create relationships from the `actors.csv` file.
In the following example, `LOAD CSV` is used with the `WITH HEADERS` option to access the fields in each `row` by their corresponding column name.

[TIP]
====
The default field delimiter for `LOAD CSV` is the comma (`,`).
Use the link:{neo4j-docs-base-uri}/cypher-manual/current/clauses/load-csv/#load-csv-import-data-from-a-csv-file-with-a-custom-field-delimiter[`FIELDTERMINATOR` option] to set a different delimiter.

If the CSV file is large, use the link:{neo4j-docs-base-uri}/cypher-manual/current/clauses/load-csv/#load-csv-importing-large-amounts-of-data[`CALL IN TRANSACTIONS` clause] to commit a number of rows per transaction instead of the whole file.
====

Furthermore:

* `MATCH` and `MERGE` are used to find nodes (taking advantage of the constraints created in the <<_create_constraints>> section) and create a relationship between them.
* `ON CREATE SET` is used to set the value of a relationship property when a new one is created.
* `RETURN count({star})` is used to show the number of processed rows.

[.tabbed-example]
====
[.include-with-GDS-client]
=====
[source, python, subs=attributes+]
----
gds.run_cypher("""
    LOAD CSV
      WITH HEADERS
      FROM 'https://data.neo4j.com/intro/movies/actors.csv' AS row
    MATCH (p:Person {name: row.person})
    MATCH (m:Movie {title: row.movie})
    MERGE (p)-[actedIn:ACTED_IN]->(m)
      ON CREATE SET actedIn.roles = split(row.roles, ';')
    RETURN count({star})
""")
----
=====

[.include-with-Cypher]
=====
[source, cypher, subs=attributes+]
----
LOAD CSV
  WITH HEADERS
  FROM 'https://data.neo4j.com/intro/movies/actors.csv' AS row
MATCH (p:Person {name: row.person})
MATCH (m:Movie {title: row.movie})
MERGE (p)-[actedIn:ACTED_IN]->(m)
  ON CREATE SET actedIn.roles = split(row.roles, ';')
RETURN count({star})
----
=====

[.include-with-Python-driver]
=====
[source, python, subs=attributes+]
----
load_actors_csv = """
    LOAD CSV
      WITH HEADERS
      FROM 'https://data.neo4j.com/intro/movies/actors.csv' AS row
    MATCH (p:Person {name: row.person})
    MATCH (m:Movie {title: row.movie})
    MERGE (p)-[actedIn:ACTED_IN]->(m)
      ON CREATE SET actedIn.roles = split(row.roles, ';')
    RETURN count({star})
"""

# Create the driver session
with driver.session() as session:
    # Load the CSV file
    session.run(load_actors_csv).data()
----
=====
====

== Run a Cypher query

Once all the nodes and relationships have been created, we can run a query to check that the data have been inserted correctly. The following query looks for movies with `Keanu Reeves`, orders them by release date and groups their titles.

[.tabbed-example]
====
[.include-with-GDS-client]
=====
[source, python, subs=attributes+]
----
gds.run_cypher("""
    MATCH (person:Person {name: "Keanu Reeves"})-[:ACTED_IN]->(movie)
    RETURN movie.released, COLLECT(movie.title) AS movies
    ORDER BY movie.released
""")
----
=====

[.include-with-Cypher]
=====
[source, cypher, subs=attributes+]
----
MATCH (person:Person {name: "Keanu Reeves"})-[:ACTED_IN]->(movie)
RETURN movie.released, COLLECT(movie.title) AS movies
ORDER BY movie.released
----
=====

[.include-with-Python-driver]
=====
[source, python, subs=attributes+]
----
query = """
    MATCH (person:Person {name: "Keanu Reeves"})-[:ACTED_IN]->(movie)
    RETURN movie.released, COLLECT(movie.title) AS movies
    ORDER BY movie.released
"""

# Create the driver session
with driver.session() as session:
    # Run the Cypher query
    result = session.run(query).data()

    # Print the formatted result
    print(json.dumps(result, indent=2))
----
=====
====

== Cleanup

When the data are no longer useful, the database can be cleaned up.

[.tabbed-example]
====
[.include-with-GDS-client]
=====
[source, python, subs=attributes+]
----
# Delete data
gds.run_cypher("""
    MATCH (n)
    DETACH DELETE n
""")
----
=====

[.include-with-Cypher]
=====
[source, cypher, subs=attributes+]
----
MATCH (n)
DETACH DELETE n
----
=====

[.include-with-Python-driver]
=====
[source, python, subs=attributes+]
----
delete_data = """
    MATCH (n)
    DETACH DELETE n
"""

# Create the driver session
with driver.session() as session:
    # Delete the data
    session.run(delete_data).data()
----
=====
====

=== Closing the connection

include::partial$aurads/close-connection.adoc[]
