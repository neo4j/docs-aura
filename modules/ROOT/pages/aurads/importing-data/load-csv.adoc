[[aurads-load-csv]]
= Loading CSV files
:description: This page describes how to load CSV files into a Neo4j AuraDS instance.

TIP: Follow along with a notebook in https://colab.research.google.com/drive/1NInLPvki2gN48QeACL7ApuyFDICkuzx4?usp=sharing[image:colab.svg[Colab,24] Google Colab^]

A CSV file can be loaded into an AuraDS instance using the https://neo4j.com/docs/cypher-manual/current/clauses/load-csv/[`LOAD CSV`^] Cypher clause. For security reasons it is not possible to load local CSV files, which must be instead publicly accessible on HTTP or HTTPS servers such as GitHub, Google Drive, and Dropbox.

In this tutorial we will load three CSV files from https://data.neo4j.com:

* `movies.csv`: a list of movies with their title, release year and a short description
* `people.csv`: a list of actors with their year of birth
* `actors.csv`: a list of acting roles, where actors are matched with the movies they had a role in

WARNING: The `LOAD CSV` command is built to handle small to medium-sized data sets, such as anything up to 10 million nodes and relationships. You should avoid using this command for any data sets exceeding this limit.

include::partial$aurads/setup.adoc[]

== Create constraints

Adding constraints before loading any data usally improves data loading performance. In fact, besides adding an integrity check, a unique constraint adds an index on a property at the same time, so that `MATCH` and `MERGE` operations during loading are faster.

WARNING: For best performance when using `MERGE` or `MATCH` with `LOAD CSV`, make sure an index or a unique constraint has been created on the property used for merging. For more information, check out the https://neo4j.com/docs/getting-started/current/cypher-intro/load-csv/#_prepare_the_database[Import data] tutorial in the Cypher documentation.

In this example we add uniqueness constraints on both movie titles and actors' names.

[.tabbed-example]
====
[.include-with-GDS-client]
=====
[source, python, subs=attributes+]
----
# Make movie titles unique
gds.run_cypher("""
CREATE CONSTRAINT ON (movie:Movie) ASSERT movie.title IS UNIQUE
""")

# Make person names unique
gds.run_cypher("""
CREATE CONSTRAINT ON (person:Person) ASSERT person.name IS UNIQUE
""")
----
=====

[.include-with-Cypher]
=====
[source, cypher, subs=attributes+]
----
CREATE CONSTRAINT ON (movie:Movie) ASSERT movie.title IS UNIQUE;
CREATE CONSTRAINT ON (person:Person) ASSERT person.name IS UNIQUE;
----
=====

[.include-with-Python-driver]
=====
[source, python, subs=attributes+]
----
movie_title_constraint = """
CREATE CONSTRAINT ON (movie:Movie) ASSERT movie.title IS UNIQUE
"""

person_name_constraint = """
CREATE CONSTRAINT ON (person:Person) ASSERT person.name IS UNIQUE
"""

# Create the driver session
with driver.session() as session:
  # Make movie titles unique
  session.run(movie_title_constraint).data()
  # Make person names unique
  session.run(person_name_constraint).data()
----
=====
====

== Add nodes from CSV files

We are now ready to load the CSV files from their URIs and create nodes from the data they contain. In the following examples, `LOAD CSV` is used with `WITH HEADERS` to access `row` fields by their corresponding column name. Furthermore:

* `MERGE` is used with the indexed properties to take advantage of the constraints created in the <<_create_constraints>> section.
* `ON CREATE SET` is used to set the value of a node property when a new one is created.
* `RETURN count(*)` is used to show the number of processed rows.

Note that the CSV files in this tutorial are curated, so some assumptions are made for simplicity. In a real-world scenario, for example, a CSV file could contain multiple rows that would attempt to assign different property values to the same node; in this case, an https://neo4j.com/docs/cypher-manual/current/clauses/merge/#merge-merge-with-on-match[`ON MATCH SET`^] clause must be added to ensure this case is dealt with appropriately.

[.tabbed-example]
====
[.include-with-GDS-client]
=====
[source, python, subs=attributes+]
----
gds.run_cypher("""
LOAD CSV WITH HEADERS FROM
  'https://data.neo4j.com/intro/movies/movies.csv' AS row
MERGE (m:Movie {title: row.title})
ON CREATE SET m.released = toInteger(row.released), m.tagline = row.tagline
RETURN count(*)
""")
----

[source, python, subs=attributes+]
----
gds.run_cypher("""
LOAD CSV WITH HEADERS FROM
  'https://data.neo4j.com/intro/movies/people.csv' AS row
MERGE (p:Person {name: row.name}) 
ON CREATE SET p.born = toInteger(row.born)
RETURN count(*)
""")
----
=====

[.include-with-Cypher]
=====
[source, cypher, subs=attributes+]
----
LOAD CSV WITH HEADERS FROM
  'https://data.neo4j.com/intro/movies/movies.csv' AS row
MERGE (m:Movie {title: row.title})
ON CREATE SET m.released = toInteger(row.released), m.tagline = row.tagline
RETURN count(*);
----

[source, cypher, subs=attributes+]
----
LOAD CSV WITH HEADERS FROM
  'https://data.neo4j.com/intro/movies/people.csv' AS row
MERGE (p:Person {name: row.name}) 
ON CREATE SET p.born = toInteger(row.born)
RETURN count(*);
----
=====

[.include-with-Python-driver]
=====
[source, python, subs=attributes+]
----
load_movies_csv = """
LOAD CSV WITH HEADERS FROM
  'https://data.neo4j.com/intro/movies/movies.csv' AS row
MERGE (m:Movie {title: row.title})
ON CREATE SET m.released = toInteger(row.released), m.tagline = row.tagline
RETURN count(*)
"""

# Create the driver session
with driver.session() as session:
  # Load the CSV file
  session.run(load_movies_csv).data()
----

[source, python, subs=attributes+]
----
load_people_csv = """
LOAD CSV WITH HEADERS FROM
  'https://data.neo4j.com/intro/movies/people.csv' AS row
MERGE (p:Person {name: row.name}) 
ON CREATE SET p.born = toInteger(row.born)
RETURN count(*)
"""

# Create the driver session
with driver.session() as session:
  # Load the CSV file
  session.run(load_people_csv).data()
----
=====
====

== Add relationships from CSV files

Similarly to what we have done for nodes, we now create relationships from the `actors.csv` file. In the following examples, `LOAD CSV` is used with:

* `WITH HEADERS` to access `row` fields by their corresponding column name;
* `USING PERIODIC COMMIT` is used to control memory usage by creating a commit every _n_ processed rows rather than after processing the whole file. This is especially useful when loading large files;
* `FIELDTERMINATOR` is used to explicitly set the field terminator character (although the comma is the default).

Furthermore:

* `MATCH` and `MERGE` are used to find nodes (taking advantage of the constraints created in the <<_create_constraints>> section) and create a relationship between them.
* `ON CREATE SET` is used to set the value of a relationship property when a new one is created.
* `RETURN count(*)` is used to show the number of processed rows.

[.tabbed-example]
====
[.include-with-GDS-client]
=====
[source, python, subs=attributes+]
----
gds.run_cypher("""
USING PERIODIC COMMIT 50
LOAD CSV WITH HEADERS FROM
  'https://data.neo4j.com/intro/movies/actors.csv' AS row
FIELDTERMINATOR ','
MATCH (p:Person {name: row.person})
MATCH (m:Movie {title: row.movie})
MERGE (p)-[actedIn:ACTED_IN]->(m)
ON CREATE SET actedIn.roles = split(row.roles, ';')
RETURN count(*)
""")
----
=====

[.include-with-Cypher]
=====
[source, cypher, subs=attributes+]
----
USING PERIODIC COMMIT 50
LOAD CSV WITH HEADERS FROM
  'https://data.neo4j.com/intro/movies/actors.csv' AS row
FIELDTERMINATOR ','
MATCH (p:Person {name: row.person})
MATCH (m:Movie {title: row.movie})
MERGE (p)-[actedIn:ACTED_IN]->(m)
ON CREATE SET actedIn.roles = split(row.roles, ';')
RETURN count(*);
----
=====

[.include-with-Python-driver]
=====
[source, python, subs=attributes+]
----
load_actors_csv = """
USING PERIODIC COMMIT 50
LOAD CSV WITH HEADERS FROM
  'https://data.neo4j.com/intro/movies/actors.csv' AS row
FIELDTERMINATOR ','
MATCH (p:Person {name: row.person})
MATCH (m:Movie {title: row.movie})
MERGE (p)-[actedIn:ACTED_IN]->(m)
ON CREATE SET actedIn.roles = split(row.roles, ';')
RETURN count(*)
"""

# Create the driver session
with driver.session() as session:
  # Load the CSV file
  session.run(load_actors_csv).data()
----
=====
====

== Cleanup

When the data are no longer useful, the database can be cleaned up.

[.tabbed-example]
====
[.include-with-GDS-client]
=====
[source, python, subs=attributes+]
----
# Delete data
gds.run_cypher("""
MATCH (n)
DETACH DELETE n
""")
----
=====

[.include-with-Cypher]
=====
[source, cypher, subs=attributes+]
----
MATCH (n)
DETACH DELETE n
----
=====

[.include-with-Python-driver]
=====
[source, python, subs=attributes+]
----
delete_data = """
MATCH (n)
DETACH DELETE n
"""

# Create the driver session
with driver.session() as session:
  # Delete the data
  session.run(delete_data).data()
----
=====
====
